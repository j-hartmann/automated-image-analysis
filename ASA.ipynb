{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "S_s5efp4HIT5"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "V583BslrG5Fj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux4u11ajG0gQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install pyannote.audio\n",
        "!pip install git+https://github.com/openai/whisper.git \n",
        "!pip install filetype"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install ffmpeg"
      ],
      "metadata": {
        "id": "PN3iuUwNaxg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & paths"
      ],
      "metadata": {
        "id": "1bHHlKYuHBSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary library\n",
        "\n",
        "# For managing audio file\n",
        "import librosa\n",
        "\n",
        "# new approach to handle sound files\n",
        "import soundfile as sf\n",
        "\n",
        "#Importing Pytorch\n",
        "import torch\n",
        "\n",
        "\n",
        "#Importing Wav2Vec\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "# import transfoerms pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as ntype_of_file\n",
        "import os\n",
        "from pyannote.audio import Pipeline\n",
        "import time\n",
        "\n",
        "# OpenAIs whisper\n",
        "import whisper\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import filetype\n",
        "import shutil\n",
        "import subprocess\n",
        "import collections\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "czJeS9ymHE0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchaudio -> session crashes when being imported with torch in one go?\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T"
      ],
      "metadata": {
        "id": "RZO8cXVGA_oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dirs & paths to data"
      ],
      "metadata": {
        "id": "5b79DgS1TiuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir chunks\n",
        "!mkdir data\n",
        "!mkdir waveforms\n",
        "!mkdir transformed_files\n",
        "!mkdir csv_out"
      ],
      "metadata": {
        "id": "ikWiTlAhTmZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "id": "LnTKGWqgTMUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"\""
      ],
      "metadata": {
        "id": "hzYibj-GS9L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models and tokenizers"
      ],
      "metadata": {
        "id": "S_s5efp4HIT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "zQPRI4bXMyje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Importing Wav2Vec pretrained model\n",
        "tokenizer_FB_wav2vec2 = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model_FB_wav2vec2 = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# models to choose from:\n",
        "# tiny, base, small, medium (5GB RAM), large (10GB RAM)\n",
        "model_OA_whisper = whisper.load_model(\"base\").to(device)\n",
        "\n",
        "# pyannote diarization pipeline\n",
        "pipeline_pyannote_diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
        "\n",
        "# emotion prediction distilroberta-base (Jochen & Samuel)\n",
        "emotion_text_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True, device=0)\n",
        "# pipeline speech to emotion (fine-tuned wav2vec)\n",
        "pipeline_ste = pipeline(model=\"harshit345/xlsr-wav2vec-speech-emotion-recognition\", return_all_scores=True, device=0)"
      ],
      "metadata": {
        "id": "G5wT0o51HH2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "IPkTuICyd_NC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_filepath_path_name_type(file_path):\n",
        "\n",
        "  '''\n",
        "  This function:\n",
        "  0. takes in a filepath\n",
        "  1. splits the filepath into path_to_file, name_of_file and type_of_file\n",
        "  2. returns them\n",
        "  '''\n",
        "\n",
        "  # base splits \n",
        "  slash_split = file_path.split(\"/\")\n",
        "  dot_split = file_path.split(\".\")\n",
        "\n",
        "  #  path of given file\n",
        "  path_of_file = \"/\".join(slash_split[0:-1]) + \"/\"\n",
        "  name_file_raw = slash_split[-1].split(\".\")\n",
        "\n",
        "  # name of the given file\n",
        "  name_of_file = name_file_raw[0]\n",
        "\n",
        "  # type of the given file\n",
        "  type_of_file = name_file_raw[-1]\n",
        "\n",
        "  return path_of_file, name_of_file, type_of_file"
      ],
      "metadata": {
        "id": "N-QsbiTgrQIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write loop that combines above functions to transcribe entire audio file\n",
        "def chunk_audio(file_path, output_path, output_type, chunks_in_seconds):\n",
        "\n",
        "  # split filepath\n",
        "  # last three bools for controlling what parts of splitted path to return\n",
        "  # -> here path and name of file\n",
        "  path_of_file, name_of_file, _ = split_filepath_path_name_type(file_path)\n",
        "\n",
        "  # check length of audio file\n",
        "  y, sr = librosa.load(file_path)\n",
        "  duration = librosa.get_duration(y, sr)\n",
        "  print(duration, int(duration))\n",
        "\n",
        "  # number of iterations we will go through\n",
        "  counter =   int(duration) // int(chunks_in_seconds)\n",
        "  print(counter)\n",
        "\n",
        "\n",
        "  # chunk audio file\n",
        "  for i in range(counter):\n",
        "    y, sr = librosa.load(file_path, offset=i*chunks_in_seconds, duration=chunks_in_seconds)\n",
        "    # save chunk\n",
        "    sf.write(output_path + name_of_file +f\"_offset_{i*chunks_in_seconds}_\" + output_type, y, sr)\n"
      ],
      "metadata": {
        "id": "w8ScxPuETMDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_encoding(file_path, output_path, output_type):\n",
        "  '''\n",
        "  This function:\n",
        "  0. Takes in an audio file(string), output_path(string), duration_in_seconds(double) to cut it to, loads it\n",
        "  1. Changes its encoding to .wav format\n",
        "  2. saves it\n",
        "  '''\n",
        "\n",
        "  # split filepath\n",
        "  # last three bools for controlling what parts of splitted path to return\n",
        "  # -> here path and name of file\n",
        "  path_of_file, name_of_file, _ = split_filepath_path_name_type(file_path)\n",
        "\n",
        "  # 0: load file\n",
        "  y, sr = librosa.load(file_path)\n",
        "  # save first duration of audio file as .wav\n",
        "  #librosa.output.write_wav(output_path + name_of_file +f'_file_trim_{duration_in_seconds}s.wav', y, sr)\n",
        "\n",
        "  # 1,2:\n",
        "  sf.write(output_path + name_of_file + output_type, y, sr)"
      ],
      "metadata": {
        "id": "iAPAXuuGuBT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_filetype(path, output_path, output_type=\".wav\"):\n",
        "\n",
        "  \"\"\"\n",
        "  This function:\n",
        "  0: takes in a path to a folder\n",
        "  1: iterates over the given files\n",
        "  2: if video: extracts waveform and saves in new folder\n",
        "  3: if audio (not waveform): converts to waveform and saves in new folder\n",
        "  4: if waveform: save in new folder\n",
        "  \"\"\"\n",
        "\n",
        "  # iterate over folder\n",
        "  for filename in os.listdir(path):\n",
        "    \n",
        "    filepath = f\"{path}{filename}\"\n",
        "    # check filetype\n",
        "    kind = filetype.guess(filepath)\n",
        "\n",
        "    # file mime\n",
        "    file_mime = kind.mime.split(\"/\")[0]\n",
        "\n",
        "    print(file_mime, kind.extension)\n",
        "\n",
        "    # is video? -> extract wav and save in data and continue\n",
        "    if file_mime == \"video\":\n",
        "      print(\"is video\")\n",
        "      # define filename\n",
        "      filename_split = filename.split(\".\")[0]\n",
        "      \n",
        "      # execute ffmpeg conversion\n",
        "      !ffmpeg -i \"{path}{filename}\" \"{output_path}{filename_split}{output_type}\"\n",
        "\n",
        "      continue\n",
        "\n",
        "    # is audio? -> convert to wav and save in data and continue\n",
        "    elif file_mime == \"audio\":\n",
        "      print(\"is audio\")\n",
        "\n",
        "      # convert audio to wav if not already in that format\n",
        "      if kind.extension != \"wav\":\n",
        "        print(\"change encoding\")\n",
        "        # change encoding to wav\n",
        "        change_encoding(filepath, output_path, output_type)\n",
        "        print(\"changed encoding\")\n",
        "\n",
        "      # if already in wav format: save in data\n",
        "      else:\n",
        "        print(\"copy wav file\")\n",
        "        shutil.copy(filepath, output_path)\n",
        "\n",
        "      continue\n",
        "\n",
        "    # is neither audio nor video? -> exclaim and continue\n",
        "    elif file_mime != \"video\" and file_mime != \"audio\":\n",
        "      print(f\"Filetype is not supported! - {filepath}\")\n",
        "      continue\n",
        "\n",
        "    # is none? exclaim and continue\n",
        "    elif kind is None:\n",
        "      print(f\"Filetype is none! - {filepath}\")\n"
      ],
      "metadata": {
        "id": "9LKHdNKhhEav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diarize(input_file):\n",
        "\n",
        "  \"\"\"\n",
        "  This function:\n",
        "  0: takes in a chunked .wav file\n",
        "  1: performs diarization on it and returns resultung data\n",
        "  \"\"\"\n",
        "  return pipeline_pyannote_diarization(input_file)"
      ],
      "metadata": {
        "id": "R2UjG-QoDtlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_pyannote_instance(instance, round_by):\n",
        "  \"\"\"\n",
        "  This function:\n",
        "  0: takes in a pyannote diarization instance\n",
        "  1: processes the containing information into a list of tuples in form (start, end, speaker), rounds start, end by round_by \n",
        "  2: returns that list\n",
        "  \"\"\"\n",
        "  return [(f\"{turn.start:.2f}\", f\"{turn.end:.2f}\", f\"{speaker}\") for turn, _, speaker in instance.itertracks(yield_label=True)]"
      ],
      "metadata": {
        "id": "C1NpqiUm9_xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(text, clip_length):\n",
        "\n",
        "  \"\"\"\n",
        "  This function: \n",
        "  0: takes in an input string\n",
        "  1: counts all words\n",
        "  2: returns wordcount, unique words and their counts, wordcount/clip_length\n",
        "  \"\"\"\n",
        "  # string into list\n",
        "  words = text.split(\" \")\n",
        "  # clean single words\n",
        "  words = [word.strip(\" ,.!?§$%&/()=+-#@€°*\") for word in words]\n",
        "\n",
        "  # initialize unique words set\n",
        "  word_set = {word:0 for word in set(words)}\n",
        "  # count words\n",
        "  for word in words:\n",
        "    word_set[word] += 1\n",
        "  # calculate number of words\n",
        "  numb_words = len(words)\n",
        "  return numb_words, word_set, numb_words/clip_length"
      ],
      "metadata": {
        "id": "6g8EAn5ANViG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio_FB_wav2vec2(input_file, sample_rate):\n",
        "\n",
        "  '''\n",
        "  This function:\n",
        "  0. takes in an audio file\n",
        "  1. changes its sample rate\n",
        "  2. tokenizes the data\n",
        "  3. feeds the data into a previously defined model\n",
        "  4. predicts and transcripts outputs\n",
        "  5. returns transcription\n",
        "  Note: needs input audio to be of smaller chunk size\n",
        "  '''\n",
        "\n",
        "  # 1.\n",
        "  audio, sr = librosa.load(input_file, sr=sample_rate)\n",
        "  # 2.\n",
        "  tokenized_values = tokenizer_FB_wav2vec2(audio, return_tensors=\"pt\").input_values\n",
        "  # 3.\n",
        "  logits = model_FB_wav2vec2(tokenized_values).logits\n",
        "  # 4.\n",
        "  prediction = torch.argmax(logits, dim=-1)\n",
        "  transcription = tokenizer_FB_wav2vec2.batch_decode(prediction)[0]\n",
        "  # 5.\n",
        "  return transcription"
      ],
      "metadata": {
        "id": "gmr4jq_rcqTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio_OA_whisper(input_file):\n",
        "\n",
        "  \"\"\"\n",
        "  This function:\n",
        "  0: takes in a path to an audio file\n",
        "  1: feeds it into the chosen whisper model \n",
        "  2: determines language and text, returns both\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # load the input_file\n",
        "  audio = whisper.load_audio(input_file)\n",
        "\n",
        "  # make log-Mel spectrogram and move to the same device as the model\n",
        "  mel = whisper.log_mel_spectrogram(audio).to(model_OA_whisper.device)\n",
        "\n",
        "  # detect the spoken language\n",
        "  _, probs = model_OA_whisper.detect_language(mel)\n",
        "\n",
        "  # decode the audio\n",
        "  options = whisper.DecodingOptions()\n",
        "\n",
        "  # get the text from returned tokens with \"ret.text\"\n",
        "  # get language prediction: max(probs, key=probs.get)\n",
        "  # returns text, predicted language and mel-spectogram\n",
        "  return whisper.decode(model_OA_whisper, mel, options), probs, mel"
      ],
      "metadata": {
        "id": "QvwrX8oetspp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# emotion prediction on extracted text\n",
        "def emotion_prediction_text(text):\n",
        "  \"\"\"\n",
        "  This function:\n",
        "  0: takes in an extracted string\n",
        "  1: performs emotion prediction on it and returns those predicted values\n",
        "  \"\"\"\n",
        "  return emotion_text_classifier(text)               "
      ],
      "metadata": {
        "id": "RWkKwTYc9IWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# general metadata extraction\n",
        "def metadata(path):\n",
        "\n",
        "  \"\"\"\n",
        "  This file:\n",
        "  0: loads a wave file\n",
        "  1: performs pitch frequency detection\n",
        "  2-(n-1): performs various other metadata extraction procedures\n",
        "  3: returns all extracted data\n",
        "  \"\"\"\n",
        "  # load file\n",
        "  wave, sr = torchaudio.load(path, normalize=True)\n",
        "\n",
        "  # detect pitch\n",
        "  # testing pitch (torchaudio): https://pytorch.org/tutorials/beginner/audio_feature_extractions_tutorial.html\n",
        "  pitch = F.detect_pitch_frequency(wave, sr)\n",
        "\n",
        "\n",
        "\n",
        "  # calculate spectogram\n",
        "  # spectogram, siehe: https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py\n",
        "  # https://en.wikipedia.org/wiki/Spectrogram\n",
        "  transform = torchaudio.transforms.Spectrogram()\n",
        "  spect = transform(wave)\n",
        "  return pitch, spect"
      ],
      "metadata": {
        "id": "P0-k6T9phezm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that searches for ad mentions\n",
        "def admention(ads, text):\n",
        "\n",
        "  \"\"\"\n",
        "  This function:\n",
        "  0: takes in a list of ads and the extracted text of a current clip\n",
        "  1: compares the extract text with the mentioned ads\n",
        "  2: returns a dictionary in {ad_1: ad_c1_counter, .., ad_n: ad_n_counter}\n",
        "  \"\"\"\n",
        "\n",
        "  ad_dict = collections.Counter(text)\n",
        "  return  [(k,v) for (k,v) in ad_dict.items() if k in ads]\n",
        "\n",
        "def process_string(text):\n",
        "\n",
        "  \"\"\"\n",
        "  This function:\n",
        "  0: takes in a string\n",
        "  1: lowers the string\n",
        "  2: strips the string of unwated chars and returns it\n",
        "  \"\"\"\n",
        "  return text.lower().strip(\" .:,;!?1234567890\")"
      ],
      "metadata": {
        "id": "y_uSnmeDgN8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note: first draft, meant to incorporate more functionality over time if useful and condense previous code\n",
        "# class to pre-process data and create a dataset instance\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, filepath, path_chunks, chunk_size):\n",
        "    self.filepath = filepath\n",
        "    self.path_chunks = path_chunks\n",
        "    self.chunk_size = chunk_size\n",
        "\n",
        "    # iterate over files in folder\n",
        "    for filename in os.listdir(filepath):\n",
        "\n",
        "      # create folder for each top-file\n",
        "      new_dir = self.path_chunks  + filename.split(\".\")[0]\n",
        "      \n",
        "      # create sub-folders, each containing the respective chunks of the file\n",
        "      os.makedirs(new_dir, exist_ok=True)\n",
        "      # chunks files and exports each chunk as a .wav file into given output folder path_chunks\n",
        "      chunk_audio(filepath + filename, f\"{new_dir}/\" , \".wav\", chunk_size)\n",
        "\n",
        "  def __getitem__(self):\n",
        "    # returns all chunks as filepaths\n",
        "    for subfolder in os.listdir(self.path_chunks):\n",
        "      for filename in os.listdir(self.path_chunks+ \"/\"+subfolder):\n",
        "        print(self.path_chunks + subfolder + \"/\" + filename)\n",
        "    return sorted([self.path_chunks + subfolder + \"/\" + filename for subfolder in os.listdir(self.path_chunks) for filename in os.listdir(self.path_chunks+ \"/\"+subfolder)])\n",
        "  \n",
        "  def __len__(self):\n",
        "    # returns the number of total chunks the datset contains\n",
        "    return len(os.listdir(self.path_chunks))\n",
        "dataset = Dataset(filepath=\"/content/drive/My Drive/Colab Notebooks/her/data/ger/\", path_chunks=\"/content/chunks4/\", chunk_size=30.0)"
      ],
      "metadata": {
        "id": "SNTRkJQpKWLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(path, path_chunks, csv_out_path, csv_name, sample_rate, chunk_length, round_by, ads):\n",
        "\n",
        "  df = pd.DataFrame(columns=[\"filename\",\n",
        "                             \"transcribed_FB_wav2vec2_text\",\n",
        "                             \"transcribe_OA_whisper_lang\",\n",
        "                             \"transcribed_OA_whisper_X_to_X\",\n",
        "                             \"transcribed_OA_whisper_X_to_en\",\n",
        "                             \"transcribe_OA_whisper_mel\",\n",
        "                             \"pyannote_diarization_start\",\n",
        "                             \"pyannote_diarization_end\",\n",
        "                             \"pyannote_diarization_speaker\",\n",
        "                             \"speaker_detected\",\n",
        "                             \"wordcount\",\n",
        "                             \"unique_words\",\n",
        "                             \"words_per_s\",\n",
        "                             \"pitch\",\n",
        "                             \"spectogram\",\n",
        "                             \"duration_in_s\",\n",
        "                             \"sample_rate\",\n",
        "                             \"size_kB\"])\n",
        "  \n",
        "  df_emotion = pd.DataFrame(columns=[\"filename\",\n",
        "                                     \"transcribed_OA_whisper\",\n",
        "                                     \"anger_text\",\n",
        "                                     \"disgust_text\",\n",
        "                                     \"fear_text\",\n",
        "                                     \"joy_text\",\n",
        "                                     \"neutral_text\",\n",
        "                                     \"sadness_text\",\n",
        "                                     \"surprise_text\",\n",
        "                                     \"disgust_audio\",\n",
        "                                     \"fear_audio\",\n",
        "                                     \"happiness_audio\",\n",
        "                                     \"sadness_audio\",\n",
        "                                     \"anger_audio\"])\n",
        "\n",
        "  # creates a custom df based on ads that are supposed to be searched for\n",
        "  df_admentions = pd.DataFrame(columns=[\"filename\"].append([col for col in ads]))\n",
        "\n",
        "  # all cut files in dir in list\n",
        "  #item_list = sorted(os.listdir(path))\n",
        "\n",
        "  # create a Dataset instance\n",
        "  dataset = Dataset(filepath=path, path_chunks=path_chunks, chunk_size=30.0)\n",
        "  print(dataset.__getitem__())\n",
        "  item_list = dataset.__getitem__()\n",
        "\n",
        "\n",
        "  # iterate over those files\n",
        "  for index in tqdm(range(len(item_list))):\n",
        "\n",
        "      # size of file in kB\n",
        "      file_size = os.path.getsize(item_list[index])/1024\n",
        "\n",
        "      # metadata extraction\n",
        "      # pitch: pitch-freq\n",
        "      # spect: spectogram\n",
        "      pitch, spect = metadata(item_list[index])\n",
        "      \n",
        "      # extract text, lang using different models\n",
        "      # FAIRs wav2vec2 960H\n",
        "      transcribed_FB_wav2vec2_text = transcribe_audio_FB_wav2vec2(item_list[index], sample_rate)\n",
        "      # OpenAIs whisper\n",
        "      transcribed_OA_whisper_text, lang, mel = transcribe_audio_OA_whisper(item_list[index])\n",
        "      # translate options for whisper \n",
        "      options = dict(language=\"en\", beam_size=5, best_of=10)\n",
        "      translate_options = dict(task=\"translate\", **options)\n",
        "\n",
        "      # if language is in en, perform adcounts on it\n",
        "      if max(lang, key=lang.get) == \"en\": \n",
        "        # split sentence into words\n",
        "        words = text_X_to_en.split(\" \")\n",
        "        print(words)\n",
        "          # call admentions - returns [(ad, count)]\n",
        "          # feed into as lower case\n",
        "        ad_list = admention(ads=list(map(lambda x: process_string(x), ads)), text=list(map(lambda x: process_string(x), words)))\n",
        "          # write into df\n",
        "        df_admentions.at[index, \"filename\"] = item_list[index].split(\"/\")[-1]\n",
        "        for (ad, number) in ad_list:\n",
        "          df_admentions.at[index, ad] = number\n",
        "        \n",
        "      \n",
        "      # if  the detected language is not in english, translate to english as well\n",
        "      if max(lang, key=lang.get) != \"en\":\n",
        "        text_X_to_en = model_OA_whisper.transcribe(item_list[index], **translate_options)[\"text\"]\n",
        "        df.at[index, \"transcribed_OA_whisper_X_to_en\"] = text_X_to_en\n",
        "        # split sentence into words\n",
        "        words = text_X_to_en.split(\" \")\n",
        "        print(words)\n",
        "        # call admentions - returns [(ad, count)]\n",
        "         # feed into as lower case\n",
        "        ad_list = admention(ads=list(map(lambda x: process_string(x), ads)), text=list(map(lambda x: process_string(x), words)))\n",
        "        # write into df\n",
        "        df_admentions.at[index, \"filename\"] = item_list[index].split(\"/\")[-1]\n",
        "        for (ad, number) in ad_list:\n",
        "          df_admentions.at[index, ad] = number\n",
        "        \n",
        "\n",
        "\n",
        "      # diarization using pyannote\n",
        "      # creates a pyannote diarization instance of the current file and returns it\n",
        "      dia_instance = diarize(item_list[index])\n",
        "      # returns a list of triples: (start, end, speaker), type: str\n",
        "      diarized_list = format_pyannote_instance(dia_instance, round_by)\n",
        "\n",
        "      # emotion prediction from audio\n",
        "      emotion_preds_audio = pipeline_ste(item_list[index])\n",
        "      df_emotion.at[index, \"disgust_audio\"] = emotion_preds_audio[0][\"score\"]\n",
        "      df_emotion.at[index, \"fear_audio\"] = emotion_preds_audio[1][\"score\"]\n",
        "      df_emotion.at[index, \"happiness_audio\"] = emotion_preds_audio[2][\"score\"]\n",
        "      df_emotion.at[index, \"sadness_audio\"] = emotion_preds_audio[3][\"score\"]\n",
        "      df_emotion.at[index, \"anger_audio\"] = emotion_preds_audio[4][\"score\"]\n",
        "\n",
        "      # wordcount, unique words, words/s\n",
        "      if max(lang, key=lang.get) == \"en\":\n",
        "        wordcount, uniques, words_per_s = count_words(text=transcribed_OA_whisper_text.text, clip_length=chunk_length)\n",
        "      else:\n",
        "        wordcount, uniques, words_per_s = count_words(text=text_X_to_en, clip_length=chunk_length)\n",
        "\n",
        "      # emotion prediction from text (english)\n",
        "      if max(lang, key=lang.get) == \"en\":\n",
        "        text = emotion_prediction_text(transcribed_OA_whisper_text.text)\n",
        "        df_emotion.at[index, \"transcribed_OA_whisper\"] = transcribed_OA_whisper_text.text\n",
        "        # if original text not in english, use translation\n",
        "      else:\n",
        "        text = emotion_prediction_text(text_X_to_en)\n",
        "        df_emotion.at[index, \"transcribed_OA_whisper\"] = text_X_to_en\n",
        "\n",
        "      # write emotion prediction into the emotion df\n",
        "      df_emotion.at[index, \"filename\"] = item_list[index].split(\"/\")[-1]\n",
        "      df_emotion.at[index, \"anger_text\"] = text[0][0][\"score\"]\n",
        "      df_emotion.at[index, \"disgust_text\"] = text[0][1][\"score\"]\n",
        "      df_emotion.at[index, \"fear_text\"] = text[0][2][\"score\"]\n",
        "      df_emotion.at[index, \"joy_text\"] = text[0][3][\"score\"]\n",
        "      df_emotion.at[index, \"neutral_text\"] = text[0][4][\"score\"]\n",
        "      df_emotion.at[index, \"sadness_text\"] = text[0][5][\"score\"]\n",
        "      df_emotion.at[index, \"surprise_text\"] = text[0][6][\"score\"]\n",
        "\n",
        "      # write data into df\n",
        "      df.at[index, \"filename\"] = item_list[index].split(\"/\")[-1]\n",
        "      df.at[index, \"transcribed_FB_wav2vec2_text\"] = transcribed_FB_wav2vec2_text\n",
        "      df.at[index, \"transcribed_OA_whisper_X_to_X\"] = transcribed_OA_whisper_text.text\n",
        "      df.at[index, \"transcribe_OA_whisper_lang\"] = max(lang, key=lang.get)\n",
        "      df.at[index, \"transcribe_OA_whisper_mel\"] = mel\n",
        "      # write diarization data into df\n",
        "      try:\n",
        "        df.at[index, \"pyannote_diarization_start\"] = [item[0] for item in diarized_list]\n",
        "        df.at[index, \"pyannote_diarization_end\"] = [item[1] for item in diarized_list]\n",
        "        df.at[index, \"pyannote_diarization_speaker\"] = [item[2] for item in diarized_list]\n",
        "        df.at[index, \"speaker_detected\"] = \"yes\"\n",
        "      except:\n",
        "        df.at[index, \"speaker_detected\"] = \"no\"\n",
        "        print(\"No speaker detected\", )\n",
        "\n",
        "      df.at[index, \"wordcount\"] = wordcount\n",
        "      df.at[index, \"unique_words\"] = uniques\n",
        "      df.at[index, \"words_per_s\"] = words_per_s\n",
        "      df.at[index, \"pitch\"] = pitch\n",
        "      df.at[index, \"spectogram\"] = str(spect)\n",
        "      df.at[index, \"duration_in_s\"] = chunk_length\n",
        "      df.at[index, \"sample_rate\"] = sample_rate\n",
        "      df.at[index, \"size_kB\"] = file_size\n",
        "\n",
        "    \n",
        "  # export df\n",
        "  df.to_csv(csv_out_path + csv_name + \".csv\")\n",
        "\n",
        "  # export emotion df\n",
        "  df_emotion.to_csv(csv_out_path + csv_name + \"_emotion\" + \".csv\")\n",
        "  \n",
        "  # export admentions df\n",
        "  df_admentions.to_csv(csv_out_path + csv_name + \"_admentions\" + \".csv\")\n",
        "\n",
        "  # uber-df\n",
        "  dfs = [df, df_emotion, df_admentions]\n",
        "  # merge dfs iteratively \n",
        "  # note: reduce reduces the iterable iteratively to a single variable/value, applying left -> right\n",
        "  d_merged = reduce(lambda left, right: pd.merge(left, right, on=[\"filename\"], how=\"outer\"), dfs).fillna(\"na\")\n",
        "  # export uber-df\n",
        "  d_merged.to_csv(csv_out_path + csv_name + \"_aggregated_data\" + \".csv\")"
      ],
      "metadata": {
        "id": "_YuFR1i8sdGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests"
      ],
      "metadata": {
        "id": "REJMcL2-cIPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test chunk audio\n",
        "start = time.time()\n",
        "chunk_len = 30.0\n",
        "chunk_audio(path,\n",
        "             \"/content/chunks/\",\n",
        "             \".wav\",\n",
        "             chunk_len)\n",
        "end = time.time()\n",
        "print(round(end-start, 3))"
      ],
      "metadata": {
        "id": "dte7KMX4uOf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test chang_encoding function\n",
        "start = time.time()\n",
        "\n",
        "change_encoding(path,\n",
        "                 \"/content/transformed_files/\",\n",
        "                 \".wav\")\n",
        "end = time.time()\n",
        "print(round(end-start, 3))"
      ],
      "metadata": {
        "id": "Wg1Voh7Mi0oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "ktF5mJhe9Y-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test: full extraction function\n",
        "start = time.time()\n",
        "\n",
        "extract(path=path, path_chunks=\"/content/chunks/\", csv_out_path=\"/content/csv_out/\", csv_name=\"ASA_extraction_V1\", sample_rate=16000, chunk_length=30.0, round_by=2, ads=[])\n",
        "\n",
        "end = time.time()\n",
        "print(round(end-start, 3))"
      ],
      "metadata": {
        "id": "5tSCSSk5uVDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = [\"A\", \"B\"]\n",
        "l2 = list(map(lambda x: x.lower(), l))\n",
        "print(l2)"
      ],
      "metadata": {
        "id": "2Vszn5JCRQr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}