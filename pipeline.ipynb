{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "uXvdqasFBGMp",
        "ANtPscAjyNs7",
        "i2q6g-5FBKQm"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j-hartmann/automated-image-analysis/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "uzANHL6DIXi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs (restart runtime after this) "
      ],
      "metadata": {
        "id": "uXvdqasFBGMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# install yolo\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "!cd yolov5\n",
        "!pip install -r requirements.txt  # install\n",
        "!cd content\n",
        "!pip install deepface\n",
        "!pip install retina-face\n",
        "!pip install mtcnn\n",
        "!pip install fpdf\n",
        "# install IQA library\n",
        "!pip install image-quality"
      ],
      "metadata": {
        "id": "WR7emBIQQDuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-image"
      ],
      "metadata": {
        "id": "jHUy70Jz4FKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ANtPscAjyNs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLQlgJ2mA72b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import re\n",
        "import torch\n",
        "from skimage import measure, segmentation\n",
        "import skimage\n",
        "import copy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import PIL.Image \n",
        "import math\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deepface import DeepFace\n",
        "from retinaface import RetinaFace\n",
        "from mtcnn import MTCNN\n",
        "from fpdf import FPDF\n",
        "import imquality.brisque as brisque\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
      ],
      "metadata": {
        "id": "TXDsOwkcR2ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link drive (optional)"
      ],
      "metadata": {
        "id": "i2q6g-5FBKQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "id": "jV_qiXQfBN5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create faces dir\n",
        "!mkdir cropped_faces\n",
        "!mkdir faces\n",
        "!mkdir minmax"
      ],
      "metadata": {
        "id": "j2NeziikKJZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paths \n",
        "Path to either drive folder or local directory (GC) -> Need to configure yourself"
      ],
      "metadata": {
        "id": "unr8NgMgCjmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path to image folder, e.g. \"/content/drive/My Drive/images/\" \n",
        "path_images = \"\"\n",
        "# optional: export min/max images for each category\n",
        "path_images_out_min_max = \"/content/minmax/\"\n",
        "# path to image folder, e.g. \"/content/drive/My Drive/csvs/\"\n",
        "path_csv_out = \"\"\n",
        "# number of images in given path\n",
        "print(len(os.listdir(path_images)))"
      ],
      "metadata": {
        "id": "jdYni9aSdnjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control variables\n",
        "Note: brisque, visualbalance (and to a lesser degree faces and demographics) will increase computation time drastically"
      ],
      "metadata": {
        "id": "bRTdbGm4pP1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# control variables for feature extraction\n",
        "start = 0 #@param {type:\"integer\", min:0}\n",
        "end =  5#@param {type:\"integer\", min:0}\n",
        "resize = 25 #@param {type:\"integer\"}\n",
        "bool_brisque = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "bool_visualbalance = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "yolo_extraction_bool = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "print_values = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "rescale_w =  300#@param {type:\"integer\", min:0}\n",
        "rescale_h = 300 #@param {type:\"integer\", min:0}\n",
        "bool_save_min_max_img = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "bool_extract_faces = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "bool_demographics = True #@param [\"False\", \"True\"] {type: \"raw\"}\n",
        "bool_mtcnn = True #@param [\"False\", \"True\"] {type: \"raw\"}\n",
        "# sharpness is an alternative to the blur_effect values\n",
        "bool_sharpness = False #@param [\"False\", \"True\"] {type: \"raw\"}\n",
        "bool_show_face = True #@param [\"False\", \"True\"] {type: \"raw\"}"
      ],
      "metadata": {
        "id": "UWwnY1zxpOY6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "wdjhAU3_E6bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: WORK IN PROGRESS\n",
        "\n",
        "# calculate the euclidean distance between two images -> symmetrical\n",
        "def euclid_dist_symmetrical(image):\n",
        "\n",
        "  '''\n",
        "  This function:\n",
        "  0. Takes in an image\n",
        "  1. calls helper function adjust_image_shape(img) to \n",
        "  adjust the shape of a given image if necessary\n",
        "  -> when col/row uneven\n",
        "  2. slices the image horizontally by height\n",
        "  3. Calculates and returns the sum of all symmetrical points between both image slices\n",
        "  '''\n",
        "\n",
        "  # extract h,w\n",
        "  height, width, channels = image.shape\n",
        "  # exit if one of width or height are not even\n",
        "  if (height % 2 != 0) or (width% 2 != 0):\n",
        "    # adjust shape of image:\n",
        "    # subtract one col or row or both from given image if it is not even\n",
        "    # reason: sub image slice need to be of same size\n",
        "    image = adjust_image_shape(image)\n",
        "\n",
        "  # cut image vertically\n",
        "  # determine value to slice image by\n",
        "  slice_by = width // 2\n",
        "  # slice array\n",
        "  slice_1 = image[:, :slice_by]\n",
        "  slice_2 = image[:, slice_by:]\n",
        "\n",
        "  # euclidean distance of rgb tuple, symmetric\n",
        "  # reference: https://en.wikipedia.org/wiki/Euclidean_distance\n",
        "  euclid_distance = 0\n",
        "  for index in range(len(slice_1)):\n",
        "    for index2 in range(len(slice_1[index])):\n",
        "        euclid_distance += math.sqrt((slice_1[index][index2][0] - slice_2[index][-index2][0])**2 + (slice_1[index][index2][1] - slice_2[index][-index2][1])**2 + (slice_1[index][index2][2] - slice_2[index][-index2][2])**2)\n",
        "       \n",
        "  return euclid_distance/(width*height)"
      ],
      "metadata": {
        "id": "bLj18jKSvGTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: WORK IN PROGRESS\n",
        "\n",
        "# calculate the diagonal symmetrical distance between images\n",
        "def euclid_dist_diagonal(image):\n",
        "\n",
        "  '''\n",
        "  This function:\n",
        "  0. Takes in an image\n",
        "  1. calls helper function adjust_image_shape(img) to \n",
        "  adjust the shape of a given image if necessary\n",
        "  -> when col/row uneven\n",
        "  2. slices the image horizontally by height\n",
        "  3. Calculates and returns the sum of all diagonals between both image slices\n",
        "  '''\n",
        "\n",
        "  # extract h,w\n",
        "  height, width, channels = image.shape\n",
        "  # exit if one of width or height are not even\n",
        "  if (height % 2 != 0) or (width% 2 != 0):\n",
        "    # adjust shape of image:\n",
        "    # subtract one col or row or both from given image if it is not even\n",
        "    # reason: sub image slice need to be of same size\n",
        "    image = adjust_image_shape(image)\n",
        "\n",
        "  # cut image vertically\n",
        "  # determine value to slice image by\n",
        "  slice_by = height // 2\n",
        "  # slice array\n",
        "  slice_1 = image[:slice_by, :]\n",
        "  slice_2 = image[slice_by:, :]\n",
        "\n",
        "  # euclidean distance of rgb tuple, symmetric\n",
        "  # reference: https://en.wikipedia.org/wiki/Euclidean_distance\n",
        "  euclid_distance = 0\n",
        "  for index in range(len(slice_1)):\n",
        "    for index2 in range(len(slice_1[index])):\n",
        "        euclid_distance += math.sqrt((slice_1[index][index2][0] - slice_2[-index][-index2][0])**2 + (slice_1[index][index2][1] - slice_2[-index][-index2][1])**2 + (slice_1[index][index2][2] - slice_2[-index][-index2][2])**2)\n",
        "  \n",
        "  return euclid_distance/(width*height)"
      ],
      "metadata": {
        "id": "rK-ioAdU2NQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for euclid_dist_diagonal/symmetrical\n",
        "def adjust_image_shape(img):\n",
        "  ''' \n",
        "  This function:\n",
        "  0. takes in an image and its h,w\n",
        "  1. if h or w of slice is uneven, subtract row or col\n",
        "  2. return adjusted image\n",
        "  '''\n",
        "\n",
        "  if img.shape[0] % 2 != 0:\n",
        "    # subtract row from height\n",
        "    img = img[:-1, :]\n",
        "  if img.shape[1] % 2 != 0:\n",
        "    # subtract col from width\n",
        "    img = img[:, :-1]\n",
        "  return img"
      ],
      "metadata": {
        "id": "ADzESUSEPT48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define MTCNN detector \n",
        "detector = MTCNN()\n",
        "\n",
        "# helper function to detect faces in images -> RetinaFace\n",
        "def _detect_faces(path_img, bool_mtcnn):\n",
        "  '''\n",
        "  This function:\n",
        "  0. Takes in a path to an img\n",
        "  1. extracts all faces (bb, landmarks) as a dict and returns it\n",
        "  '''\n",
        "  \n",
        "  if bool_mtcnn:\n",
        "\n",
        "    # check if image is in RGB or grayscale format\n",
        "    # cv2.imread to not throw an error, therefore have to check if image has been read in\n",
        "    # note: reads in image with 3 channels by default as BGR, even if grayscale\n",
        "    # read image\n",
        "    img = cv2.cvtColor(cv2.imread(path_img), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    \n",
        "    # predict faces\n",
        "    faces = detector.detect_faces(img)\n",
        "\n",
        "    # reformat datastructure\n",
        "    # if list not empty\n",
        "    faces_dict = {}\n",
        "    if bool(faces):\n",
        "  \n",
        "      # iterate over list of dicts\n",
        "      counter = 1\n",
        "      for entry in faces:\n",
        "        faces_dict[\"face_\" + str(counter)] = {\"score\":entry[\"confidence\"],\"facial_area\":entry[\"box\"], \"landmarks\": {\"right_eye\": entry[\"keypoints\"][\"right_eye\"],\"left_eye\":entry[\"keypoints\"][\"left_eye\"], \"nose\":entry[\"keypoints\"][\"nose\"], \"mouth_right\":entry[\"keypoints\"][\"mouth_right\"], \"mouth_left\":entry[\"keypoints\"][\"mouth_left\"]}}\n",
        "        counter +=1\n",
        "\n",
        "    return faces_dict\n",
        "  else:\n",
        "    return RetinaFace.detect_faces(path_img)"
      ],
      "metadata": {
        "id": "9dkyaUPPAe6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper funciton to analyze faces -> DeepFace\n",
        "def _analyze_faces(path):\n",
        "  '''\n",
        "  This function:\n",
        "  0. Takes in a cropped face from a previously extracted face detection function\n",
        "  1. analyzes it and returns the extraction information as a dict (race, sex, age)\n",
        "  '''\n",
        "  return DeepFace.analyze(img_path = path, detector_backend=\"retinaface\", actions = ['age', 'gender', 'race', 'emotion'], enforce_detection=False)"
      ],
      "metadata": {
        "id": "lP7WZl-5Grgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class for writing pdfs of min/max images\n",
        "class PDF(FPDF):\n",
        "  # header of pdf\n",
        "  def header(self):\n",
        "    # define header of pdf\n",
        "    self.set_font(\"Arial\", \"B\", 20)\n",
        "    self.cell(80)\n",
        "    # align = C -> center\n",
        "    self.cell(w=30,h=5, txt=\"AIA: MinMax\", border=0, ln=0, align=\"C\")\n",
        "    # line break after title\n",
        "    self.ln(50)\n",
        "  \n",
        "  # foter of pdf\n",
        "  def footer(self):\n",
        "    # position 2cm from bottom\n",
        "    self.set_y(-20)\n",
        "    # set font\n",
        "    # no style -> normal\n",
        "    self.set_font(family=\"Arial\", style=\"\", size=10)\n",
        "    # number of page\n",
        "    # page_no() print current page number\n",
        "    self.cell(w=0, h=12, txt=\"\" + str(self.page_no()), border=0, ln=0, align='R')"
      ],
      "metadata": {
        "id": "tnFUZ-DUSXAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _iter(path_images, path_csv_out, resize_percent, start, end, bool_brisque, bool_visualbalance, yolo_extraction_bool,bool_mtcnn,bool_sharpness, extract_faces, demographics, print_values, bool_show_face):\n",
        "\n",
        "  '''\n",
        "  This function:\n",
        "  0. [x] Takes in input/output paths \n",
        "  1. [x] reads input csv\n",
        "  2. [x] iterates over given image path\n",
        "  3. [x] reads images\n",
        "  4. [x] performs feature extraction\n",
        "  5. [x] merges extracted features with base file\n",
        "  6. [x] exports new file\n",
        "  7. [x] yolov5 coco inference\n",
        "  8. [x] add # image features\n",
        "  '''\n",
        "\n",
        "  df_faces = pd.DataFrame(columns=[\"filename\",\n",
        "                                   \"face\",\n",
        "                                   \"score\",\n",
        "                                   \"facial_area\",\n",
        "                                   \"landmarks_right_eye\",\n",
        "                                   \"landmarks_left_eye\",\n",
        "                                   \"landmarks_nose\",\n",
        "                                   \"landmarks_mouth_right\",\n",
        "                                   \"landmarks_mouth_left\"])\n",
        "  \n",
        "  df_demographics = pd.DataFrame(columns=[\"filename\",\n",
        "                                          \"face\",\n",
        "                                          \"age\",\n",
        "                                          \"sex\",\n",
        "                                          \"asian\",\n",
        "                                          \"indian\",\n",
        "                                          \"black\",\n",
        "                                          \"white\",\n",
        "                                          \"middle_eastern\",\n",
        "                                          \"latino_hispanic\",\n",
        "                                          \"angry\",\n",
        "                                          \"disgust\",\n",
        "                                          \"fear\",\n",
        "                                          \"happy\",\n",
        "                                          \"sad\",\n",
        "                                          \"surprise\",\n",
        "                                          \"neutral\"])\n",
        "\n",
        "  # structure new df\n",
        "  df_new = pd.DataFrame(columns=[\"filename\",\n",
        "                                 \"height\",\n",
        "                                 \"width\",\n",
        "                                 \"r_mean\",\n",
        "                                 \"g_mean\",\n",
        "                                 \"b_mean\",\n",
        "                                 \"hue_avg\",\n",
        "                                 \"saturation_avg\",\n",
        "                                 \"brightness_avg\", \n",
        "                                 \"greyscale_avg\",\n",
        "                                 \"shannon_entropy\",\n",
        "                                 \"blur_effect_grey\",\n",
        "                                 \"blur_effect_rgb\",\n",
        "                                 \"felzenszwalb_unique_segments_grey\",\n",
        "                                 \"felzenszwalb_unique_segments_rgb\",\n",
        "                                 \"size_kB\",\n",
        "                                 \"visual_complexity_kB\",\n",
        "                                 \"visual_balance_euclid_dist_symmetrical\",\n",
        "                                 \"visual_balance_euclid_dist_diagonal\",\n",
        "                                 \"image_clarity\",\n",
        "                                 \"sharpness\",\n",
        "                                 \"brisque_IQA\",\n",
        "                                 \"warm_hues_perc\",\n",
        "                                 \"cold_hues_perc\",\n",
        "                                 \"xmin\", \n",
        "                                 \"ymin\",\n",
        "                                 \"xmax\",\n",
        "                                 \"ymax\", \n",
        "                                 \"confidence\", \n",
        "                                 \"label\"])\n",
        "  \n",
        "  # read names of given image directory into list\n",
        "  # slice into array that we want to analyze\n",
        "  names = os.listdir(path_images)[start:end]\n",
        "\n",
        "  # faces counter\n",
        "  faces_counter = 0\n",
        "\n",
        "  # iterate over zipped data -> [(x1,y1,z1),...,(xn,yn,zn)]\n",
        "  for index, name in enumerate(names):\n",
        "\n",
        "          print(f\"\\n\\nImage {index}/{end-start} from {start} to {end}\")\n",
        "\n",
        "          # read image as BGR\n",
        "          # reads in grayscale with 3 channels per default\n",
        "          try:\n",
        "            img = cv2.imread(path_images + name)\n",
        "          except:\n",
        "            # skip image if empty file read in\n",
        "            df_faces.at[faces_counter, \"filename\"] =  name\n",
        "            df_demographics.at[faces_counter, \"filename\"] =  name\n",
        "            df_new.at[index, \"filename\"] =  name\n",
        "            continue\n",
        "\n",
        "\n",
        "          # check if read file is not empty\n",
        "          if np.shape(img) == () or img.shape == None:\n",
        "            # skip image if empty file read in\n",
        "            df_faces.at[faces_counter, \"filename\"] =  name\n",
        "            df_demographics.at[faces_counter, \"filename\"] =  name\n",
        "            df_new.at[index, \"filename\"] =  name\n",
        "            continue\n",
        "\n",
        "          # if faces are supposed to be extracted\n",
        "          faces = {}\n",
        "\n",
        "          # id of image\n",
        "          image_id = name.split(\".\")[0].split(\"_\")[-1]\n",
        "          \n",
        "          if extract_faces:\n",
        "\n",
        "            # workaround, if face detector returns empty set\n",
        "            faces_empty = False\n",
        "            # call helper function to extract faces\n",
        "\n",
        "            faces = _detect_faces(path_img=path_images + name, bool_mtcnn=bool_mtcnn)\n",
        "\n",
        "            try:\n",
        "              if \"face_1\" in faces.keys():\n",
        "                faces_empty = True\n",
        "                #print(\"detected face\", faces_empty)\n",
        "            except:\n",
        "              print(\"Demographics detector unable to detect faces\")\n",
        "\n",
        "            # check if set is empty\n",
        "            if faces_empty ==  True:\n",
        "              # if set is not empty, iterate over detected faces and write down the collected data\n",
        "              for face in faces:\n",
        "                print(f\"\\nCurrent face (of {index}/{end-start}): {face}\")\n",
        "                # write filename of detected face\n",
        "                df_faces.at[faces_counter, \"filename\"] =  name\n",
        "                df_faces.at[faces_counter, \"face\"] = face\n",
        "                # write down score of detected face\n",
        "    \n",
        "                df_faces.at[faces_counter, \"score\"] = str(faces[face][\"score\"])\n",
        "                # write down rect values of detected face\n",
        "                facial_area = faces[face][\"facial_area\"]\n",
        "                df_faces.at[faces_counter, \"facial_area\"] = str(facial_area)\n",
        "                # write down landmarks right eye, left eye, nose, mouthr/l\n",
        "                df_faces.at[faces_counter, \"landmarks_right_eye\"] = str(faces[face][\"landmarks\"][\"right_eye\"])\n",
        "                df_faces.at[faces_counter, \"landmarks_left_eye\"] = str(faces[face][\"landmarks\"][\"left_eye\"])\n",
        "                df_faces.at[faces_counter, \"landmarks_nose\"] = str(faces[face][\"landmarks\"][\"nose\"])\n",
        "                df_faces.at[faces_counter, \"landmarks_mouth_right\"] = str(faces[face][\"landmarks\"][\"mouth_right\"])\n",
        "                df_faces.at[faces_counter, \"landmarks_mouth_left\"] = str(faces[face][\"landmarks\"][\"mouth_left\"])\n",
        "\n",
        "\n",
        "                ## refactor: crop, write, resize and show face outside of demographics block\n",
        "                # crop faces from img and save them (function requires path, not byte obj)\n",
        "                tmp = img.copy()\n",
        "                  # crop face out of the copy -> [y:y+h, x:x+w] \n",
        "                  # width: x2-x1, height: y2-y1\n",
        "                if bool_mtcnn == False:\n",
        "                    # retinaFace: x1,y1,x2,y2\n",
        "                  tmp_face = tmp[facial_area[1]:facial_area[1] +facial_area[3]-facial_area[1], facial_area[0]:facial_area[0] +facial_area[2]-facial_area[0]]\n",
        "                else:\n",
        "                    # mtcnn: x,y,w,h\n",
        "                    # so: [x:x+w, y:y+h]\n",
        "                  tmp_face = tmp[facial_area[1]:facial_area[1] +facial_area[3], facial_area[0]:facial_area[0] +facial_area[2]]\n",
        "\n",
        "                  # resize it\n",
        "                tmp_face = cv2.resize(tmp_face, (224,224))\n",
        "                 \n",
        "                  # show image if control var is true\n",
        "                if bool_show_face:\n",
        "                  cv2_imshow(tmp_face)\n",
        "     \n",
        "                # if bool_demographics == True\n",
        "                if demographics:\n",
        "\n",
        "                   # save it\n",
        "                  name_split = name.split(\".\")\n",
        "                  path_crop = \"/content/cropped_faces/\" + name_split[0] + \"_\" + face + \".\" + name_split[1]\n",
        "                  cv2.imwrite(path_crop, tmp_face)\n",
        "                  \n",
        "                  try:\n",
        "                      # call demographics function\n",
        "                    demographics_data = _analyze_faces(path_crop)\n",
        "\n",
        "                      # write values into df\n",
        "                    df_demographics.at[faces_counter, \"filename\"] = name\n",
        "                    df_demographics.at[faces_counter, \"face\"] = face\n",
        "                    df_demographics.at[faces_counter, \"age\"] = demographics_data[\"age\"]\n",
        "                    df_demographics.at[faces_counter, \"sex\"] = demographics_data[\"gender\"]\n",
        "                    df_demographics.at[faces_counter, \"asian\"] = demographics_data[\"race\"][\"asian\"]\n",
        "                    df_demographics.at[faces_counter, \"indian\"] = demographics_data[\"race\"][\"indian\"]\n",
        "                    df_demographics.at[faces_counter, \"black\"] = demographics_data[\"race\"][\"black\"]\n",
        "                    df_demographics.at[faces_counter, \"white\"] = demographics_data[\"race\"][\"white\"]\n",
        "                    df_demographics.at[faces_counter, \"middle_eastern\"] = demographics_data[\"race\"][\"middle eastern\"]\n",
        "                    df_demographics.at[faces_counter, \"latino_hispanic\"] = demographics_data[\"race\"][\"latino hispanic\"]\n",
        "                    df_demographics.at[faces_counter, \"angry\"] = demographics_data[\"emotion\"][\"angry\"]\n",
        "                    df_demographics.at[faces_counter, \"disgust\"] = demographics_data[\"emotion\"][\"disgust\"]\n",
        "                    df_demographics.at[faces_counter, \"fear\"] = demographics_data[\"emotion\"][\"fear\"]\n",
        "                    df_demographics.at[faces_counter, \"happy\"] = demographics_data[\"emotion\"][\"happy\"]\n",
        "                    df_demographics.at[faces_counter, \"sad\"] = demographics_data[\"emotion\"][\"sad\"]\n",
        "                    df_demographics.at[faces_counter, \"surprise\"] = demographics_data[\"emotion\"][\"surprise\"]\n",
        "                    df_demographics.at[faces_counter, \"neutral\"] = demographics_data[\"emotion\"][\"neutral\"]\n",
        "                  except:\n",
        "                     #write values into df\n",
        "                    df_demographics.at[faces_counter, \"filename\"] =  name\n",
        "                    df_demographics.at[faces_counter, \"face\"] = face\n",
        "                    df_demographics.at[faces_counter, \"age\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"gender\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"asian\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"indian\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"black\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"white\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"middle_eastern\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"latino hispanic\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"angry\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"disgust\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"fear\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"happy\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"sad\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"surprise\"] = \"na\"\n",
        "                    df_demographics.at[faces_counter, \"neutral\"] = \"na\"\n",
        "                faces_counter += 1\n",
        "\n",
        "          # since read in image differs in output val when read with PIL/cv2, choose PIL as stated in documentation\n",
        "          pil_image = PIL.Image.open(path_images + name)\n",
        "\n",
        "          # resize image\n",
        "          output = cv2.resize(img, (int(img.shape[1] * resize_percent / 100), int(img.shape[0] * resize_percent / 100)))\n",
        "\n",
        "          # brisque val\n",
        "          brisque_val = 0\n",
        "          \n",
        "          # perform brisque computation\n",
        "          if bool_brisque:\n",
        "            try:\n",
        "              brisque_val = brisque.score(output)\n",
        "            except:\n",
        "              print(\"brisque error\")\n",
        "\n",
        "            if print_values:\n",
        "              print(f\"brisque={brisque_val}\")\n",
        "\n",
        "          sharpness = \"\"\n",
        "          # sharpness\n",
        "          if bool_sharpness:\n",
        "            sharpness = cv2.Laplacian(img, cv2.CV_64F).var()\n",
        "            if print_values:\n",
        "              print(f\"sharpness={sharpness}\")\n",
        "\n",
        "          # get height and width\n",
        "          height, width, channel = img.shape\n",
        "          if print_values:\n",
        "            print(\"height, width\",height, width)\n",
        "          \n",
        "          # convert to RGB\n",
        "          img_rgb = cv2.imread(path_images + name, cv2.COLOR_BGR2RGB)\n",
        "          \n",
        "          is_rgb = False\n",
        "          # check if image in rgb format\n",
        "          if len(img_rgb.shape) > 2:\n",
        "            is_rgb = True\n",
        "\n",
        "\n",
        "\n",
        "          # calculate the symmetrical euclidean distance of a given image\n",
        "          # important: have to verify if this follows description from paper\n",
        "          euclid_dist_sym_bool = True\n",
        "          euclid_dist_diag_bool = True\n",
        "          if bool_visualbalance:\n",
        "    \n",
        "            try:\n",
        "              euclid_dist_sym  = euclid_dist_symmetrical(img_rgb)\n",
        "            except:\n",
        "              euclid_dist_sym_bool = False\n",
        "              print(\"error: uneven image\")\n",
        "\n",
        "            # euclid dist diagonal visual balance\n",
        "            # own idea, needs to be verified and testet if useful\n",
        "            \n",
        "            try:\n",
        "              euclid_dist_diag = euclid_dist_diagonal(img_rgb)\n",
        "            except:\n",
        "              euclid_dist_diag_bool = False\n",
        "              print(\"error: uneven image\")\n",
        "\n",
        "          # split image in RGB format into single color changels\n",
        "          try:\n",
        "            r,g,b = cv2.split(img_rgb)\n",
        "            # take mean\n",
        "            r_mean = r.mean()\n",
        "            g_mean = g.mean()\n",
        "            b_mean = b.mean()\n",
        "\n",
        "            if print_values:\n",
        "              print(f\"r={r_mean}\\nb={g_mean}\\nb={b_mean}\")\n",
        "\n",
        "              # means of colors\n",
        "            df_new.at[index, \"r_mean\"] = r_mean\n",
        "            df_new.at[index, \"g_mean\"] = g_mean\n",
        "            df_new.at[index, \"b_mean\"] = b_mean\n",
        "          except:\n",
        "            print(\"Error: unable to extract RGB-colors\")\n",
        "            # write na's into rgb cols if read image is not in rgb format\n",
        "            df_new.at[index, \"r_mean\"] = \"na\"\n",
        "            df_new.at[index, \"g_mean\"] = \"na\"\n",
        "            df_new.at[index, \"b_mean\"] = \"na\"\n",
        "\n",
        "          # convert to HSV-map\n",
        "          if is_rgb:\n",
        "            img_hsv = cv2.imread(path_images + name, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "            # average of values\n",
        "            hue = img_hsv[0].mean()\n",
        "            if print_values:\n",
        "              print(f\"hue mean={hue}\")\n",
        "            saturation = img_hsv[1].mean()\n",
        "            if print_values:\n",
        "              print(f\"saturation mean={saturation}\")\n",
        "            brightness = img_hsv[2].mean()\n",
        "            if print_values:\n",
        "              print(f\"brightness mean={brightness}\")\n",
        "\n",
        "            # calculate proportion of warm hues\n",
        "            hues = img_hsv[0]\n",
        "            if print_values:\n",
        "              print(f\"hues={hues}\")\n",
        "            # flatten hue values\n",
        "            hues_flattened = [item for sublist in hues for item in sublist]\n",
        "            # filter warm hues\n",
        "            warm_hues = [item for item in hues_flattened if item >= 30 and item <= 110]\n",
        "            cold_hues = [item for item in hues_flattened if item < 30 or item > 110]\n",
        "            if print_values:\n",
        "              print(f\"warm hues={warm_hues}\")\n",
        "              print(f\"cold hues={cold_hues}\")\n",
        "\n",
        "            # percentage of image that is of warm hue\n",
        "            warm_hues_perc = len(warm_hues) / len(hues_flattened)\n",
        "            if print_values:\n",
        "              print(f\"warm hues perc={warm_hues_perc}\")\n",
        "            cold_hues_perc = len(cold_hues) / len(hues_flattened)\n",
        "            if print_values:\n",
        "              print(f\"cold_hues_perc={cold_hues_perc}\")\n",
        "\n",
        "            df_new.at[index, \"hue_avg\"] = hue\n",
        "            df_new.at[index, \"warm_hues_perc\"] = warm_hues_perc\n",
        "            df_new.at[index, \"cold_hues_perc\"] = cold_hues_perc\n",
        "            df_new.at[index, \"saturation_avg\"] = saturation\n",
        "            df_new.at[index, \"brightness_avg\"] = brightness\n",
        "          else:\n",
        "            df_new.at[index, \"hue_avg\"] = \"na\"\n",
        "            df_new.at[index, \"warm_hues_perc\"] = \"na\"\n",
        "            df_new.at[index, \"cold_hues_perc\"] = \"na\"\n",
        "            df_new.at[index, \"saturation_avg\"] = \"na\"\n",
        "            df_new.at[index, \"brightness_avg\"] = \"na\"\n",
        "\n",
        "\n",
        "          \n",
        "          # define scaler\n",
        "          try:\n",
        "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "            # normalize brightness to [0-1]\n",
        "            normalize_brightness = scaler.fit_transform(img_hsv[2])\n",
        "            if print_values:\n",
        "              print(f\"normalize_brightness={normalize_brightness}\")\n",
        "\n",
        "            # flatten nested list\n",
        "            brightness_flattened = [item for sublist in normalize_brightness for item in sublist]\n",
        "            if print_values:\n",
        "              print(f\"flattened={brightness_flattened}\")\n",
        "\n",
        "            # count bright values -> [0,1] as described in paper\n",
        "            # https://pubsonline.informs.org/doi/suppl/10.1287/mnsc.2021.4175/suppl_file/mnsc.2021.4175.sm1.pdf\n",
        "            bright_values = [item for item in brightness_flattened if item >= 0.7]\n",
        "\n",
        "            # image clarity\n",
        "            image_clarity = len(bright_values)/len(brightness_flattened)\n",
        "            if print_values:\n",
        "              print(f\"image_clarity={image_clarity}\")\n",
        "            \n",
        "            # image clarity\n",
        "            df_new.at[index, \"image_clarity\"] = image_clarity\n",
        "          except:\n",
        "            print(\"Error: could not compute image clarity\")\n",
        "            # image clarity\n",
        "            df_new.at[index, \"image_clarity\"] = \"na\"\n",
        "\n",
        "          # convert to grayscale\n",
        "          img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "          # calculate contrast of image\n",
        "          # we choose RMS contrast, which is the SD of a gray image\n",
        "          RMS_contrast = img_grey.std()\n",
        "          if print_values:\n",
        "            print(f\"RMS_contrast={RMS_contrast}\")\n",
        "\n",
        "          # average of grayscale\n",
        "          avg_grey = np.average(img_grey)\n",
        "          if print_values:\n",
        "            print(f\"avg_gray={avg_grey}\")\n",
        "          \n",
        "          \n",
        "\n",
        "\n",
        "          # object detection inference yolov5 (choose model earlier)\n",
        "          predictions = []\n",
        "          if yolo_extraction_bool:\n",
        "            predictions = model(img).pandas().xyxy[0]\n",
        "            if print_values:\n",
        "              print(f\"predictions={predictions}\")\n",
        "\n",
        "            # number of detected objects -> one xmin (arbitrary value) for each found bb\n",
        "            numb_objects = len(predictions[\"xmin\"])\n",
        "            if print_values:\n",
        "              print(f\"numb_objects={numb_objects}\")        \n",
        "\n",
        "          # file size in kB -> file read in as # in bit\n",
        "          file_size_kB = int(os.path.getsize(path_images + name) /(1024 * 8))\n",
        "          if print_values:\n",
        "            print(f\"file size in bit={file_size_kB}\")\n",
        "\n",
        "          # visual complexity kB\n",
        "          visual_complexity_kB = file_size_kB / (width*height)\n",
        "          if print_values:\n",
        "            print(f\"visual complexity kB={visual_complexity_kB}\")\n",
        "\n",
        "          # Shannon-Entropy\n",
        "          try:\n",
        "            shannon_entropy = measure.shannon_entropy(img_grey)\n",
        "            if print_values:\n",
        "              print(f\"shannon-entropy={shannon_entropy}\")\n",
        "          except:\n",
        "            print(\"Error: could not compute shannon entropy\")\n",
        "\n",
        "          # felzenszwalb image segmentation -> computes # of unqiue segments in given image\n",
        "          # ref: https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.felzenszwalb\n",
        "          felzenszwalb_unique_grey = len(np.unique(segmentation.felzenszwalb(img_grey)))\n",
        "          if is_rgb:\n",
        "            felzenszwalb_unique_rgb = len(np.unique(segmentation.felzenszwalb(img_rgb, channel_axis=2)))\n",
        "\n",
        "            if print_values:\n",
        "              print(f\"unique segments grey = {felzenszwalb_unique_grey} and unique segments rgb = {felzenszwalb_unique_rgb}\")\n",
        "\n",
        "            df_new.at[index, \"felzenszwalb_unique_segments_rgb\"] = felzenszwalb_unique_rgb\n",
        "\n",
        "          else:\n",
        "            df_new.at[index, \"felzenszwalb_unique_segments_rgb\"] = \"na\"\n",
        "          \n",
        "      \n",
        "          # calculates the blur effect of an image (0,1)\n",
        "          # 0 lowest, 1 highest blur\n",
        "          # ref: https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.blur_effect\n",
        "          # blur on greyscale\n",
        "          try:\n",
        "            blur_effect_val = measure.blur_effect(img_grey)\n",
        "          except:\n",
        "            print(\"Error: could not compute blur effect\")\n",
        "\n",
        "          # blur efffect on rgb, channel_axis indicates color axis\n",
        "          if is_rgb:\n",
        "            blur_effect_rgb = measure.blur_effect(img_rgb, channel_axis=2)\n",
        "            if print_values:\n",
        "              print(f\"blur effect value={blur_effect_val}\")\n",
        "              print(f\"blur grey = {blur_effect_val} and blur rgb = {blur_effect_rgb}\")\n",
        "\n",
        "            df_new.at[index, \"blur_effect_rgb\"] = blur_effect_rgb\n",
        "          else:\n",
        "            df_new.at[index, \"blur_effect_rgb\"] = \"na\"\n",
        "\n",
        "\n",
        "          # write values\"\n",
        "          df_new.at[index, \"filename\"] = name\n",
        "          df_new.at[index, \"greyscale_avg\"] = avg_grey\n",
        "          df_new.at[index, \"width\"] = width\n",
        "          df_new.at[index, \"height\"] = height\n",
        "          df_new.at[index, \"shannon_entropy\"] = shannon_entropy\n",
        "          df_new.at[index, \"blur_effect_grey\"] = blur_effect_val\n",
        "          df_new.at[index, \"felzenszwalb_unique_segments_grey\"] = felzenszwalb_unique_grey\n",
        "          df_new.at[index, \"visual_complexity_kB\"] = visual_complexity_kB\n",
        "          df_new.at[index, \"size_kB\"] = file_size_kB\n",
        "\n",
        "          # sharpness\n",
        "          df_new.at[index, \"sharpness\"] = sharpness\n",
        "\n",
        "          # RMS contrast\n",
        "          df_new.at[index, \"RMS_contrast\"] = RMS_contrast\n",
        "\n",
        "          # brisq val\n",
        "          if bool_brisque:\n",
        "            df_new.at[index, \"brisque_IQA\"] = brisque_val\n",
        "          else:\n",
        "            df_new.at[index, \"brisque_IQA\"] = \"na\"\n",
        "\n",
        "          # euclidean average distance between pixels across the \n",
        "          # symmetrical (split image) vertical line\n",
        "          if euclid_dist_sym_bool and bool_visualbalance:\n",
        "            df_new.at[index, \"visual_balance_euclid_dist_symmetrical\"] = euclid_dist_sym\n",
        "          \n",
        "          else:\n",
        "            df_new.at[index, \"visual_balance_euclid_dist_symmetrical\"] = \"na\"\n",
        "          \n",
        "          # euclidean average distance of diagonals\n",
        "          # if even image\n",
        "          if euclid_dist_diag_bool and bool_visualbalance:\n",
        "            df_new.at[index, \"visual_balance_euclid_dist_diagonal\"] = euclid_dist_diag\n",
        "          \n",
        "          else:\n",
        "              df_new.at[index, \"visual_balance_euclid_dist_diagonal\"] = \"na\"\n",
        "            \n",
        "          # if predicitons is not empty\n",
        "          if yolo_extraction_bool == True:\n",
        "            #if predictions.empty() == False:\n",
        "              # write most predictions in df\n",
        "              df_new.at[index,\"xmin\"] = predictions[\"xmin\"].to_list()\n",
        "              df_new.at[index,\"ymin\"] = predictions[\"ymin\"].to_list()\n",
        "              df_new.at[index,\"xmax\"] = predictions[\"xmax\"].to_list()\n",
        "              df_new.at[index,\"ymax\"] = predictions[\"ymax\"].to_list()\n",
        "              df_new.at[index,\"confidence\"] = predictions[\"confidence\"].to_list()\n",
        "              df_new.at[index,\"label\"] = predictions[\"name\"].to_list()\n",
        "          else:\n",
        "            df_new.at[index,\"xmin\"] = \"na\"\n",
        "            df_new.at[index,\"ymin\"] = \"na\"\n",
        "            df_new.at[index,\"xmax\"] = \"na\"\n",
        "            df_new.at[index,\"ymax\"] = \"na\"\n",
        "            df_new.at[index,\"confidence\"] = \"na\"\n",
        "            df_new.at[index,\"label\"] = \"na\"\n",
        "\n",
        "          \n",
        "  # drop unnamed if existent\n",
        "  if df_new.columns[0] == \"Unnamed: 0\":\n",
        "    df_new = df_new.drop(\"Unnamed: 0\", axis=1)\n",
        "\n",
        "  # define date\n",
        "  date = datetime.datetime.now()\n",
        "\n",
        "  # export merged files\n",
        "  df_new.to_csv(path_csv_out + f\"{date.year}-{date.month}-{date.day}-Pipeline_CV_features_start_{start}_end_{end}.csv\")\n",
        "  df_demographics.to_csv(path_csv_out + f\"{date.year}-{date.month}-{date.day}-Pipeline_CV_demographics_start_{start}_end_{end}.csv\")\n",
        "  df_faces.to_csv(path_csv_out + f\"{date.year}-{date.month}-{date.day}-Pipeline_CV_faces_start_{start}_end_{end}.csv\")"
      ],
      "metadata": {
        "id": "Ybdm6WvEEMOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read full csv in\n",
        "# with filename\n",
        "def investigate(rescale_width, rescale_height, filepath, img_out):\n",
        "  \n",
        "  # read previously compute feature file\n",
        "  df = pd.read_csv(filepath)\n",
        "\n",
        "\n",
        "  # store names in list\n",
        "  names = df[\"filename\"].to_list()\n",
        "  # drop non-numeric values\n",
        "  if bool_visualbalance:\n",
        "    df = df.drop([\"Unnamed: 0\",\"filename\", \"ymax\", \"ymin\", \"xmax\", \"xmin\", \"confidence\", \"label\"], axis=1)\n",
        "  else:\n",
        "    df = df.drop([\"Unnamed: 0\",\"filename\", \"ymax\", \"ymin\", \"xmax\", \"xmin\",\"visual_balance_euclid_dist_symmetrical\", \"visual_balance_euclid_dist_diagonal\", \"confidence\", \"label\"], axis=1)\n",
        "  if bool_brisque == False:\n",
        "    df = df.drop(\"brisque_IQA\", axis=1)\n",
        "  if bool_sharpness == False:\n",
        "    df = df.drop(\"sharpness\", axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    # set \"na\" to NaN vlaues\n",
        "  df.replace('na', pd.NA, inplace=True)\n",
        "  # drop all rows containing NaNs -> this will eliminate grayscale images\n",
        "  df.dropna(how=\"any\", inplace=True)\n",
        "\n",
        "  # convert values to numeric types\n",
        "  df.apply(pd.to_numeric)\n",
        "\n",
        "\n",
        "\n",
        "  if bool_save_min_max_img:\n",
        "    # initialize object\n",
        "    pdf = PDF()\n",
        "    # create a page\n",
        "    pdf.add_page()\n",
        "\n",
        "  counter = 0\n",
        "  counter_mult = 0\n",
        "  # zip indices of min, max with columns in df2\n",
        "  # iterate over them\n",
        "  \n",
        "  print(df.idxmin())\n",
        "  for indexmin, indexmax, col in zip(df.idxmin(), df.idxmax(), df.columns):\n",
        "\n",
        "    min_name = names[int(indexmin)]\n",
        "    max_name = names[int(indexmax)]\n",
        "\n",
        "\n",
        "    # read in min image\n",
        "    imagemin = cv2.imread(path_images + min_name)\n",
        "    # resize min image to default value\n",
        "    image_min_scaled = cv2.resize(imagemin, (rescale_width, rescale_height))\n",
        "    # read in max image\n",
        "    imagemax = cv2.imread(path_images + max_name)\n",
        "    # rescale max image to default value\n",
        "    image_max_scaled = cv2.resize(imagemax, (rescale_width, rescale_height))\n",
        "\n",
        "    min_val = df.at[int(indexmin), col]\n",
        "    max_val = df.at[int(indexmax), col]\n",
        "\n",
        "    double_window =  np.concatenate((image_min_scaled, image_max_scaled), axis=1)\n",
        "    print(f\"\\nFeature:  {col}\\nLeft(min) value: {min_val} image: {min_name}\")\n",
        "    print(f\"Right(max) value: {max_val} image: {max_name}\")\n",
        "    cv2_imshow(double_window)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    # if save min/max selected\n",
        "    if bool_save_min_max_img:\n",
        "      # save min\n",
        "      cv2.imwrite(img_out + col + \"_min_\" + min_name, imagemin)\n",
        "      # save max\n",
        "      cv2.imwrite(img_out + col + \"_max_\" + max_name, imagemax)\n",
        "\n",
        "\n",
        "      # write values slightly above images\n",
        "      pdf.set_y(20 + (counter_mult * 67.5))\n",
        "      pdf.set_font('Arial', '', 10)\n",
        "      pdf.cell(w=0, h = 0, txt = str(min_val), border = 0, ln = 0,align = 'L', fill = False)\n",
        "      pdf.cell(w=0, h = 0, txt = str(max_val), border = 0, ln = 0,align = 'R', fill = False)\n",
        "\n",
        "\n",
        "      # write images into df\n",
        "      pdf.image(img_out + col + \"_min_\" + min_name, x=11, y=25 + (counter_mult * 67.5), w=50, h=50)\n",
        "      pdf.image(img_out + col + \"_max_\" + max_name, x=150, y=25 + (counter_mult * 67.5), w=50, h=50)\n",
        "      \n",
        "      # write col on same y level\n",
        "      pdf.set_y(20 + (counter_mult * 67.5))\n",
        "      pdf.set_font('Arial', '', 10)\n",
        "      pdf.cell(w=0, h = 0, txt = col, border = 0, ln = 0,align = 'C', fill = False, link = '')\n",
        "\n",
        "      # increase counters\n",
        "      counter += 1\n",
        "      counter_mult += 1\n",
        "      # if eight images have been added\n",
        "      # add new page\n",
        "      if counter % 4 == 0 and counter != 0:\n",
        "        pdf.add_page()\n",
        "        counter_mult = 0\n",
        "\n",
        "  # write pdf\n",
        "  if bool_save_min_max_img:\n",
        "    pdf.output(path_csv_out + 'MinMax_images.pdf', 'F')  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4TgUI0fOWfgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze images"
      ],
      "metadata": {
        "id": "HBSB-P7Z4w8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# time function call\n",
        "starttime = time.time()\n",
        "# function call\n",
        "_iter(path_images=path_images,\n",
        "      path_csv_out=path_csv_out,\n",
        "      resize_percent = resize,\n",
        "      start=start,\n",
        "      end=end,\n",
        "      bool_brisque=bool_brisque,\n",
        "      bool_visualbalance=bool_visualbalance,\n",
        "      yolo_extraction_bool = yolo_extraction_bool,\n",
        "      bool_mtcnn = bool_mtcnn,\n",
        "      bool_sharpness=bool_sharpness,\n",
        "      extract_faces = bool_extract_faces,\n",
        "      demographics = bool_demographics,\n",
        "      print_values=print_values,\n",
        "      bool_show_face=bool_show_face)\n",
        "\n",
        "endtime = time.time()\n",
        "print(f\"Elapsed time in seconds: {round(endtime-starttime, 3)}\")"
      ],
      "metadata": {
        "id": "uW7GKyIWV550"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect extracted features (open to see outputs)"
      ],
      "metadata": {
        "id": "4V2N8k0kuD2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read extracted feature file\n",
        "# drop cols with NaN values\n",
        "# define date - change path to file if you are running this on a different day than having the previous feature extraction performed on\n",
        "date = datetime.datetime.now()\n",
        "df = pd.read_csv(path_csv_out + f\"{date.year}-{date.month}-{date.day}-Pipeline_CV_features_start_{start}_end_{end}.csv\")\n",
        "\n",
        "# drop cols containing NaNs\n",
        "if bool_brisque == False:\n",
        "  df = df.drop(\"brisque_IQA\", axis=1)\n",
        "if bool_visualbalance == False:\n",
        "  df = df.drop([\"visual_balance_euclid_dist_symmetrical\",\"visual_balance_euclid_dist_diagonal\"], axis=1)\n",
        "if bool_sharpness == False:\n",
        "  df = df.drop(\"sharpness\", axis=1)\n",
        "if yolo_extraction_bool == False:\n",
        "  df = df.drop([\"ymax\",\"ymin\", \"xmax\", \"xmin\", \"confidence\", \"label\"], axis=1)\n",
        "if df.columns[0] == \"Unnamed: 0\":\n",
        "  df = df.drop(\"Unnamed: 0\", axis=1)\n",
        "\n",
        "\n",
        "# set \"na\" to NaN vlaues\n",
        "df.replace('na', pd.NA, inplace=True)\n",
        "# drop all rows containing NaNs -> this will eliminate grayscale images\n",
        "df = df.dropna(how=\"any\")\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "_mdStIzWu_VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Investigate max/min feature values (open to see outputs)\n",
        "Note: Since images might be too small or too big, they will be resized to a default value of 300."
      ],
      "metadata": {
        "id": "OYXFAK-cBXgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# investigates last file containing extracted features\n",
        "#investigate(rescale_width=rescale_w,rescale_height=rescale_h,filepath=path_csv_out + f\"{date.year}-{date.month}-{date.day}-Pipeline_CV_features_start_{start}_end_{end}.csv\",img_out=path_images_out_min_max)"
      ],
      "metadata": {
        "id": "8qgeWZaWGNmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}